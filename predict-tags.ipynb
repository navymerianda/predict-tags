{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main entry to the code. Contros the overall flow of this program.\n",
    "def main():\n",
    "    try:\n",
    "        #All the function calls are disabled as they have to be run in a certain sequence.\n",
    "        \n",
    "        CreateDatabaseFileFromTrainingSet()\n",
    "        numberOfRows = TotalRowsInTrainSet()\n",
    "        duplicatesData = CheckForDuplicates()\n",
    "        ClearDuplicatesAndCreateNewDataFile(duplicatesData)\n",
    "        print('Total Duplicates : ' + str(numberOfRows - duplicatesData.shape[0]))\n",
    "        tagsData = GetTagsData()\n",
    "        tagsDataModel = GetUniqueTagsAndTagsDictionary(tagsData)\n",
    "        ExploreAndPlotTagsData(tagsDataModel)\n",
    "        GetDataAfterPreprocessing()\n",
    "        pData = GetPreprocessedData()\n",
    "        questionsExplained = ConvertTagsToMultiOpVariables(pData)\n",
    "        Featurize(pData)\n",
    "        \n",
    "    except Exception as GeneralException:\n",
    "        print(GeneralException)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the sqllite database file from training set.\n",
    "def CreateDatabaseFileFromTrainingSet():\n",
    "    if not os.path.isfile('train.db'):\n",
    "        disk_engine = create_engine('sqlite:///train.db')\n",
    "        start = dt.datetime.now()\n",
    "        chunkLength = 100000\n",
    "        totalCount = 0\n",
    "        index_start = 1\n",
    "        for dataFrame in pd.read_csv('Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize = chunkLength, iterator=True, encoding='utf-8', ):\n",
    "            dataFrame.index += index_start\n",
    "            totalCount += 1\n",
    "            print('{} rows'.format(totalCount * chunkLength))\n",
    "            dataFrame.to_sql('data', disk_engine, if_exists='append')\n",
    "            index_start = dataFrame.index[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the total rows from the table created above.\n",
    "def TotalRowsInTrainSet():\n",
    "    if os.path.isfile('train.db'):\n",
    "        con = sqlite3.connect('train.db')\n",
    "        numberOfRows = pd.read_sql_query(\"\"\"SELECT count(Id) FROM data\"\"\", con)\n",
    "        print(\"Number of rows in the database :\",\"\\n\", numberOfRows['count(Id)'].values[0])\n",
    "        con.close()\n",
    "        return numberOfRows.values[0]\n",
    "    else:\n",
    "        print(\"Please check if the train.db file exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for duplicates using the group by clause in the above table.\n",
    "def CheckForDuplicates():\n",
    "    if os.path.isfile('train.db'):\n",
    "        start = datetime.now()\n",
    "        con = sqlite3.connect('train.db')\n",
    "        duplicatesCount = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as Duplicate_Count FROM data GROUP BY Title, Body, Tags', con)\n",
    "        con.close()\n",
    "        return duplicatesCount\n",
    "        print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "    else:\n",
    "        print(\"Please check if the train.db file exists!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clears up the duplicates found in the table and creates a new table with no duplicates for further processing.\n",
    "def ClearDuplicatesAndCreateNewDataFile(duplicatesData):\n",
    "    duplicatesData.dropna(inplace=True)\n",
    "    if not os.path.isfile('trainwithoutduplicates.db'):\n",
    "        diskDuplicates = create_engine(\"sqlite:///trainwithoutduplicates.db\")\n",
    "        noDuplicates = pd.DataFrame(duplicatesData, columns=['Title', 'Body', 'Tags'])\n",
    "        noDuplicates.to_sql('NoDuplicateTrain', diskDuplicates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns all the tags found in the above table.\n",
    "def GetTagsData():\n",
    "    if os.path.isfile('trainwithoutduplicates.db'):\n",
    "        con = sqlite3.connect('trainwithoutduplicates.db')\n",
    "        tagsData = pd.read_sql_query(\"\"\"SELECT Tags FROM NoDuplicateTrain\"\"\", con)\n",
    "        con.close()\n",
    "        tagsData.drop(tagsData.index[0], inplace=True)\n",
    "        tagsData.head()\n",
    "        return tagsData\n",
    "    else:\n",
    "        print(\"Please check if the trainwithoutduplicates.db file exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputs total tags and unique tags count using a vectorizer module and creates a dictionary with tag and associated count.\n",
    "def GetUniqueTagsAndTagsDictionary(tagsData):\n",
    "    vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "    tagsDataModel = vectorizer.fit_transform(tagsData['Tags'])\n",
    "    print(\"Total Tags :\" + str(tagsDataModel.shape[0]))\n",
    "    print(\"Unique Tags :\" + str(tagsDataModel.shape[1]))\n",
    "    tags = vectorizer.get_feature_names()\n",
    "    \n",
    "    freqs = tagsDataModel.sum(axis = 0).A1\n",
    "    result = dict(zip(tags, freqs))\n",
    "    if not os.path.isfile('TagsCountDictionary.csv'):\n",
    "        with open('TagsCountDictionary.csv', 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for key, value in result.items():\n",
    "                writer.writerow([key, value])\n",
    "    return tagsDataModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the tags count dictionary, retrieve some insight into tags.\n",
    "def ExploreAndPlotTagsData(tagsDataModel):\n",
    "    tagsDataframe = pd.read_csv(\"TagsCountDictionary.csv\", names=['Tags', 'Counts'])\n",
    "    tagsDataframe.head()\n",
    "    \n",
    "    tagsDataframeSorted = tagsDataframe.sort_values(['Counts'], ascending=False)\n",
    "    print(tagsDataframeSorted[0:20])\n",
    "    tagsCount = tagsDataframeSorted['Counts'].values\n",
    "    \n",
    "    tagsAgainstQuestionsCount = tagsDataModel.sum(axis = 1).tolist()\n",
    "    tagsAgainstQuestionsCount = [int(jTag) for iCount in tagsAgainstQuestionsCount for jTag in iCount]\n",
    "    print ('We have total {} Data Rows.'.format(len(tagsAgainstQuestionsCount)))\n",
    "    print(tagsAgainstQuestionsCount[:5])\n",
    "    \n",
    "    print( \"Max No of Tags Per Question: %d\"%max(tagsAgainstQuestionsCount))\n",
    "    print( \"Min No of Tags Per Question: %d\"%min(tagsAgainstQuestionsCount))\n",
    "    print( \"Avg. No of Tags Per Question: %f\"% ((sum(tagsAgainstQuestionsCount)*1.0)/len(tagsAgainstQuestionsCount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data - start point.\n",
    "def PreprocessData():\n",
    "    if os.path.isfile('trainwithoutduplicates.db'):\n",
    "        connection = CreateConnection('trainwithoutduplicates.db')\n",
    "    if connection is not None:\n",
    "        allDataConn = connection.cursor()\n",
    "        allDataConn.execute(\"SELECT Title, Body, Tags From NoDuplicateTrain LIMIT 200001;\")\n",
    "        PreprocessQuestionsFromBody(allDataConn)\n",
    "    \n",
    "    allDataConn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex to remove special characters.\n",
    "def RemoveHtmlTags(data):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(data))\n",
    "    return cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entire preprocess data in entailed in this function.\n",
    "def PreprocessQuestionsFromBody(allData):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    englishStemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    questions_with_code=0\n",
    "    len_pre=0\n",
    "    len_post=0\n",
    "    questions_proccesed = 0\n",
    "    writeQuestionsConn = CreateConnection('PreprocessedQues.db')\n",
    "    writeQuestions = writeQuestionsConn.cursor()\n",
    "    for row in allData:\n",
    "        is_code = 0\n",
    "        title, question, tags = row[0], row[1], str(row[2])\n",
    "        if '<code>' in question:\n",
    "            questions_with_code+=1\n",
    "            is_code = 1\n",
    "            x = len(question)+len(title)\n",
    "            len_pre+=x\n",
    "            \n",
    "            code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "            \n",
    "            question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "            question = RemoveHtmlTags(question.encode('utf-8'))\n",
    "        \n",
    "        title=title.encode('utf-8')\n",
    "        question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "\n",
    "        question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
    "        words=word_tokenize(str(question.lower()))\n",
    "        \n",
    "\n",
    "        question=' '.join(str(englishStemmer.stem(j)) for j in words if j not in stopWords and (len(j)!=1 or j=='c'))\n",
    "        \n",
    "        len_post+=len(question)\n",
    "        tupleData = (question,code,tags,x,len(question),is_code)\n",
    "        questions_proccesed += 1\n",
    "        writeQuestions.execute(\"INSERT INTO QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\", tupleData)\n",
    "\n",
    "        \n",
    "    writeQuestionsConn.commit()\n",
    "    writeQuestionsConn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve processed data after removing all the characters that don't support our findings.\n",
    "def GetPreprocessedData():\n",
    "    if os.path.isfile('PreprocessedQues.db'):\n",
    "        connection = CreateConnection('PreprocessedQues.db')\n",
    "    if connection is not None:\n",
    "        preprocessedData = pd.read_sql_query(\"\"\"SELECT question, tags FROM QuestionsProcessed\"\"\", connection)\n",
    "        \n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "    \n",
    "    return preprocessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tags to numerical array.\n",
    "def ConvertTagsToMultiOpVariables(pData):\n",
    "    vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "    multilabelY = vectorizer.fit_transform(pData['tags'])\n",
    "    \n",
    "    questionsExplained = []\n",
    "    total_tags = multilabelY.shape[1]\n",
    "    total_qs = pData.shape[0]\n",
    "    for i in range(500, total_tags, 100):\n",
    "        questionsExplained.append(np.round(((total_qs-QuestionExplained(i, pData))/total_qs)*100,3))\n",
    "    \n",
    "    print(\"with \",5500,\"tags we are covering \",questionsExplained[50],\"% of questions\")\n",
    "    print(\"with \",500,\"tags we are covering \",questionsExplained[0],\"% of questions\")\n",
    "    return questionsExplained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return specific tags.\n",
    "def TagsSelected(n, pData):\n",
    "    vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "    multilabelY = vectorizer.fit_transform(pData['tags'])\n",
    "    t = multilabelY.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabelYn = multilabelY[:,sorted_tags_i[:n]]\n",
    "    return multilabelYn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionExplained(n, pData):\n",
    "    multilabelYn = TagsSelected(n, pData)\n",
    "    x= multilabelYn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to featurize data and test performance.\n",
    "def Featurize(pData):\n",
    "    multilabelYX = TagsSelected(500, pData)\n",
    "    trainDataSize = 160000\n",
    "    Xtrain = pData.head(trainDataSize)\n",
    "    Xtest = pData.tail(pData.shape[0] - 160000)\n",
    "    \n",
    "    Ytrain = multilabelYX[0:trainDataSize,:]\n",
    "    Ytest = multilabelYX[trainDataSize:pData.shape[0],:]\n",
    "    \n",
    "    print(\"Number of data rows in training data :\", Ytrain.shape)\n",
    "    print(\"Number of data rows in testing data :\", Ytest.shape)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=0.00009, max_features=40000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,2))\n",
    "    \n",
    "    XTrainMultilabel = vectorizer.fit_transform(Xtrain['question'])\n",
    "    XTestMultilabel = vectorizer.transform(Xtest['question'])\n",
    "    \n",
    "    classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "    classifier.fit(XTrainMultilabel, Ytrain)\n",
    "    predictions = classifier.predict (XTestMultilabel)\n",
    "\n",
    "\n",
    "    print(\"Accuracy :\",metrics.accuracy_score(Ytest, predictions))\n",
    "    print(\"Hamming loss \",metrics.hamming_loss(Ytest,predictions))\n",
    "\n",
    "\n",
    "    precisionMicroScore = precision_score(Ytest, predictions, average='micro')\n",
    "    recallMicroScore = recall_score(Ytest, predictions, average='micro')\n",
    "    f1MicroScore = f1_score(Ytest, predictions, average='micro')\n",
    "    \n",
    "    print(\"Micro Average\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precisionMicroScore, recallMicroScore, f1MicroScore))\n",
    "\n",
    "    precisionMacroScore = precision_score(Ytest, predictions, average='macro')\n",
    "    recallMacroScore = recall_score(Ytest, predictions, average='macro')\n",
    "    f1MacroScore = f1_score(Ytest, predictions, average='macro')\n",
    "    \n",
    "    print(\"Macro Average\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precisionMacroScore, recallMacroScore, f1MacroScore))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create database connection.   \n",
    "def CreateConnection(databaseFile):\n",
    "    try:\n",
    "        conn = sqlite3.connect(databaseFile)\n",
    "        return conn\n",
    "    except Exception as ConnError:\n",
    "        print(ConnError)\n",
    "    return None\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
